{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression and basis functions\n",
    "\n",
    "**Contents**\n",
    "\n",
    "1. Load datasets\n",
    "2. Univariate multiple linear regression\n",
    "   1. Idea\n",
    "   2. Finding the best straight line\n",
    "   3. Notations for univariate, multiple linear regression\n",
    "   4. Illustration: Univariate simple linear regression\n",
    "   5. Simple linear regression on non-linear data\n",
    "3. Basis functions\n",
    "   1. Linear regression with polynomial basis functions\n",
    "   2. Linear regression with polynomial basis functions on non-polynomial data\n",
    "   3. Linear regression with cosine basis functions\n",
    "   4. Linear regression with gaussian basis functions\n",
    "4. Choose the right basis functions and the right parameters: model selection and evaluation\n",
    "   1. Model selection\n",
    "   2. Model evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data01 = np.load(\"data/data01.npy\")\n",
    "X01, Y01 = data01[:,0], data01[:,1]\n",
    "data02 = np.load(\"data/data02.npy\")\n",
    "X02, Y02 = data02[:,0], data02[:,1]\n",
    "data03 = np.load(\"data/data03.npy\")\n",
    "X03, Y03 = data03[:,0], data03[:,1]\n",
    "data04 = np.load(\"data/data04.npy\")\n",
    "X04, Y04 = data04[:,0], data04[:,1]\n",
    "\n",
    "def sort_wrt_x_axis(X,Y, return_idx=False):\n",
    "    \"\"\"\n",
    "    Sort X and Y with respect to X, usefull for better plots\n",
    "    \"\"\"\n",
    "    idx = np.argsort(X)\n",
    "    if return_idx:\n",
    "        res = (X[idx], Y[idx], idx)\n",
    "    else:\n",
    "        res = (X[idx], Y[idx])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Univariate multiple linear regression\n",
    "\n",
    "#### 2.1 Idea\n",
    "\n",
    "Given a point cloud defined by $n$ pairs $(\\mathbf{x_i}, y_i)$ where $\\mathbf{x_i} \\in \\mathbb{R}^{d} $ are features (or predictors) and $y_i \\in \\mathbb{R}$ are observed values.\n",
    "The objective of univariate linear regression is to find the straight line that best fits to this point cloud, i.e find the line's parameters $\\mathbf{w} \\in \\mathbb{R}^{d} $ (for *weights*) such that $\\textrm{SSE} = \\sum_i \\epsilon_i^2$ is minimal, with \n",
    "\n",
    "$$y_i = \\mathbf{w}^T \\mathbf{x_i}  + \\epsilon_i$$\n",
    "\n",
    "The term *univariate* refers to each $y_i$ being scalars, and the term *multiple* refers to each $\\mathbf{x_i}$ being vectors.\n",
    "\n",
    "#### 2.2 Finding the best straight line\n",
    "\n",
    "By computing the gradient, setting it to $0$ and solving the equation, we can show that this $\\textrm{SSE}$ error is minimal for $$\\hat{\\mathbf{w}} = \\mathbf{X}^+\\mathbf{y}$$ where $\\mathbf{X}^+$ is the pseudo-inverse of $\\mathbf{X}$, i.e  $\\mathbf{X}^+ = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T$\n",
    "\n",
    "Once $\\hat{w}$ has been computed, it can be used on new feature values $\\mathbf{X}_{new}$ to predict $\\mathbf{y}_{pred}$ with $$\\mathbf{y}_{pred} = \\hat{\\mathbf{w}}^T \\mathbf{X}_{new}$$\n",
    "\n",
    "#### 2.3 Notations for **univariate**, **multiple** linear regression\n",
    "\n",
    "- $n$ input vectors $\\mathbf{x}_{i=1..n}$ in a $d+1$ dimensional space representing $n$ samples of $d$ features (+ 1 dummy variable for the biais, also called intercept) , i.e \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{x}_i = \\begin{bmatrix}1, x_{i,1}, x_{i,2}, \\cdots, x_{i,d}\\end{bmatrix}^T \\in \\mathbb{R}^{d}  \\qquad \\textrm{and}  \\;\n",
    "\\mathbf{X} = \n",
    "\\begin{bmatrix}\n",
    "1 & x_{1,1} & x_{1,2} & \\cdots & x_{1, d} \\\\\n",
    "1 & x_{2,1} & x_{2,2} & \\cdots & x_{2, d} \\\\\n",
    "\\vdots & \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "1 & x_{n,1} & x_{n,2} & \\cdots & x_{n, d}\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{(n, d+1)}\n",
    "\\end{equation}\n",
    "\n",
    "- $n$ output values $\\mathbf{y}_{i=1..n}$ (one for each input) representing $n$ observations in $\\mathbb{R}$\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{y} = \n",
    "\\begin{bmatrix}y_{1}, y_{2}, \\cdots,y_{n}\n",
    "\\end{bmatrix}^T \\in \\mathbb{R}^{n}\n",
    "\\end{equation}\n",
    "\n",
    "- $d+1$ unknown parameters or regression coefficients $\\mathbf{w}_{k=0..d}$ (one for each feature + biais) representing the slope associated to the $k^{th}$ feature\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{w} = \n",
    "\\begin{bmatrix}w_{0}, w_{1}, \\cdots & w_{d} \\end{bmatrix}^T\\in \\mathbb{R}^{d+1}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dummy_variable(x):\n",
    "    return np.column_stack([np.ones(len(x)), x])\n",
    "\n",
    "def fit_LR(x,y, add_biais=True):\n",
    "    \"\"\"\n",
    "    Solve the linear regression problem, i.e compute W_hat\n",
    "    \"\"\"\n",
    "    # Add dummy variable if not already done\n",
    "    if add_biais:\n",
    "        x = add_dummy_variable(x)\n",
    "    # Compute w = pseudo_inv(X) * y (using matrix multiplication)\n",
    "    w = np.linalg.pinv(x) @ y\n",
    "    return w\n",
    "\n",
    "def predict_LR(x, w, add_biais=True):\n",
    "    \"\"\"\n",
    "    Returns ``Y_pred`` given new features values and a weights\n",
    "    \"\"\"\n",
    "    X = np.copy(x)\n",
    "    # Add dummy variable if not already done\n",
    "    if add_biais:\n",
    "        X = add_dummy_variable(X)\n",
    "    # Returns y_pred = W.T*X (using matrix multiplication)\n",
    "    return np.array([w.T @ xi for xi in X])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Illustration: Univariate simple linear regression\n",
    "\n",
    "If $d$ = 1 (i.e one feature), the regression is simple.\n",
    "\n",
    "Questions:\n",
    "\n",
    "1. Datasets \n",
    "   1. Using the function [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) from scikit-learn library, split ``X01`` and ``Y01`` into 3 datasets, a training one, a validation one and a testing one with a ratio 0.6, 0.2, 0.2. **HINT**: take a look at how we did it in the solution of Homework 1!\n",
    "   2. Print the length (number of datapoints) of each datasets using [len()](https://docs.python.org/3/library/functions.html#len) or [ndarray.shape](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.shape.html)\n",
    "2. Fitting your model\n",
    "   1. Call ``fit_LR`` on your training dataset to compute the weights (store them in ``W``)\n",
    "   2. What do ``W[0]`` and ``W[1]`` represent?\n",
    "3. Predictions\n",
    "   1. Call  ``predict_LR`` on your validation dataset and store the output in ``Y01_pred``\n",
    "4. Now using scikit-learn:\n",
    "   1. Instantiate an object of the class [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn-linear-model-linearregression) from the scikit-learn library and call it ```regr```\n",
    "   2. Apply the [fit()](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.fit) method using your training dataset.\n",
    "   3. Apply the [predict()](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.predict) method using your validation dataset.\n",
    "   4. Take a look at how the mse (Mean Square Error) of ``Y01_pred`` is computed and then compute the mse of ``Y01_pred_scikit``\n",
    "   5. Take a look at how how the coefficient of determination $R^2$ is computed and then compute $R^2$ of ``Y01_pred_scikit``. ([Wikipedia](https://en.wikipedia.org/wiki/Coefficient_of_determination): \"*$R^2$ is the proportion of the variance in the dependent variable that is predictable from the independent variable(s).  It provides a measure of how well observed outcomes are replicated by the model, based on the proportion of total variation of outcomes explained by the model\"*)\n",
    "   6. Compare with the Mean Square Error and the coefficient of determination of ``Y01_pred``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" ============ Split datasets ============\")\n",
    "\n",
    "# Fix random seed for reproducibility\n",
    "seed = 666                    \n",
    "# Shuffle and split the data into train and a concatenation of validation and test sets\n",
    "X01_train, X01_val_test, Y01_train, Y01_val_test = ... #TODO!\n",
    "\n",
    "seed = 221\n",
    "# Shuffle and split the data into validation and test sets\n",
    "X01_val, X01_test, Y01_val, Y01_test = ...             #TODO!                                                                          \n",
    "\n",
    "# sort with respect to x_axis for better plots\n",
    "X01_train, Y01_train = sort_wrt_x_axis(X01_train, Y01_train)\n",
    "X01_val, Y01_val = sort_wrt_x_axis(X01_val, Y01_val)\n",
    "X01_test, Y01_test = sort_wrt_x_axis(X01_test, Y01_test)\n",
    "\n",
    "\n",
    "# Store number of datapoints in each dataset:\n",
    "N_train = len(Y01_train)\n",
    "N_val = ...  #TODO!  \n",
    "N_test = ... #TODO!  \n",
    "print(\"Datapoints used for training:   \", N_train)\n",
    "print(\"Datapoints used for validation: \", N_val)\n",
    "print(\"Datapoints used for testing :   \", N_test)\n",
    "\n",
    "print(\" ============ Using Formula ============\")\n",
    "print(\" --- Fit the linear model ---\")\n",
    "# ----------------------\n",
    "# Training\n",
    "# ----------------------\n",
    "W = fit_LR(..., ..., add_biais=True) #TODO!  \n",
    "print(\"W_0 : %.2f\" %W[0])\n",
    "print(\"W_1 : %.2f\" %W[1])\n",
    "# Evaluating performance on training data\n",
    "Y01_pred = predict_LR(X01_train, W, add_biais=True)\n",
    "print(\"Mean Square Error: %.2f\" %mean_squared_error(Y01_train, Y01_pred))\n",
    "print('Coefficient of determination: %.2f' %r2_score(Y01_train, Y01_pred))\n",
    "# ----------------------\n",
    "# Plots\n",
    "# ----------------------\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.title(\"Univariate simple regression - Using pseudo inverse\")\n",
    "plt.scatter(X01_train, Y01_train, label=\"Training data\")\n",
    "plt.plot(X01_train, Y01_pred, c=\"r\", marker=\"\", label=\"Linear regression\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\" --- Predictions ---\")\n",
    "# ----------------------\n",
    "# Evaluation\n",
    "# ----------------------\n",
    "Y01_pred = predict_LR(X01_val, W)\n",
    "# Performance on validation data\n",
    "print(\"Mean Square Error: %.2f\" %mean_squared_error(Y01_val, Y01_pred))\n",
    "print('Coefficient of determination: %.2f' %r2_score(Y01_val, Y01_pred))\n",
    "\n",
    "\n",
    "print(\" ============ Using scikit-learn ============ \")\n",
    "\n",
    "# Instantiate an object of the class sklearn class \"LinearRegression\"\n",
    "regr = ...                                        #TODO!  \n",
    "print(\" --- Fit the linear model ---\")\n",
    "# ----------------------\n",
    "# Training\n",
    "# ----------------------\n",
    "# Note: You might need to reshape your data either using array.reshape(-1, 1) if \n",
    "# your data has a single feature when calling the fit method.\n",
    "regr.fit(...)                                     #TODO!\n",
    "\n",
    "print(\"W_0 : %.2f\" %regr.intercept_[0])\n",
    "print(\"W_1 : %.2f\" %regr.coef_[0,0])\n",
    "\n",
    "print(\" --- Predictions ---\")\n",
    "# Note: You might need to reshape your data either using array.reshape(-1, 1) if \n",
    "# your data has a single feature when calling the predict method.\n",
    "Y01_pred_scikit = regr.predict(...)               #TODO!\n",
    "print(\"Mean Square Error: %.2f\" %...)             #TODO!\n",
    "print('Coefficient of determination: %.2f' %...)  #TODO!\n",
    "# ----------------------\n",
    "# Plots\n",
    "# ----------------------\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.title(\"Univariate simple regression - Comparison with sklearn model\")\n",
    "plt.scatter(X01_train,Y01_train, label=\"Training data\")\n",
    "plt.scatter(X01_val,Y01_val, label=\"Expected values\")\n",
    "plt.plot(X01_val,Y01_pred, c=\"r\", marker=\"\", ls=\"--\",lw=1, label=\"Linear regression\")\n",
    "plt.plot(X01_val, Y01_pred_scikit, c=\"g\", marker=\"\", ls=\":\", lw=1, label=\"Scikit-learn model\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Simple linear regression on non-linear data\n",
    "\n",
    "1. Datasets \n",
    "   1. Using the function [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) from scikit-learn library, split ``X02`` and ``Y02`` into 3 datasets, a training one, a validation one and a testing one with a ratio 0.6, 0.2, 0.2.\n",
    "2. Fitting your model\n",
    "   1. Call ``fit_LR`` on your training dataset to compute the weights (store them in ``W``)\n",
    "3. Predictions\n",
    "   1. Call  ``predict_LR`` on your validation dataset and store the output in ``Y02_pred``\n",
    "   2. Compute the Mean Squared Error (MSE) of ``Y02_pred``\n",
    "   3. Computed the coefficient of determination $R^2$ of ``Y02_pred``\n",
    "4. Interpretation\n",
    "   1. Do you think that the linear regression works well here? Why?\n",
    "   2. If you had to compare the performance of this linear regression with the performance of the linear regression in the cell above, would you rather look at the mean squared error or the coefficient of determination? Why?\n",
    "   3. Would a bigger training dataset improve the perfomance? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" ============ Split datasets ============\")\n",
    "\n",
    "seed = 666                    # Fix random seed for reproducibility\n",
    "# Shuffle and split the data into train and a concatenation of validation and test sets\n",
    "X02_train, X02_val_test, Y02_train, Y02_val_test = ... #TODO!\n",
    "seed = 221\n",
    "# Shuffle and split the data into validation and test sets\n",
    "X02_val, X02_test, Y02_val, Y02_test = ... #TODO!\n",
    "\n",
    "# sort with respect to x_axis for better plots\n",
    "X02_train, Y02_train = sort_wrt_x_axis(X02_train, Y02_train)\n",
    "X02_val, Y02_val = sort_wrt_x_axis(X02_val, Y02_val)\n",
    "X02_test, Y02_test = sort_wrt_x_axis(X02_test, Y02_test)\n",
    "\n",
    "print(\"Datapoints used for training:   \", ...)    #TODO!\n",
    "print(\"Datapoints used for validation: \", ...)    #TODO!\n",
    "print(\"Datapoints used for testing :   \", ...)    #TODO!\n",
    "\n",
    "print(\" ============ Linear regression ============\")\n",
    "# ----------------------\n",
    "# Training\n",
    "# ----------------------\n",
    "... = fit_LR(..., ...)             #TODO!\n",
    "print(\"W_0 : %.2f\" %W[0])\n",
    "print(\"W_1 : %.2f\" %W[1])\n",
    "# ----------------------\n",
    "# Evaluation\n",
    "# ----------------------\n",
    "... = predict_LR(...)                            #TODO!\n",
    "print(\"Mean Square Error: %.2f\" %...)            #TODO!\n",
    "print('Coefficient of determination: %.2f' %...) #TODO!\n",
    "# ----------------------\n",
    "# Plots\n",
    "# ----------------------\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title(\"Linear regression on non-linear data\")\n",
    "plt.scatter(X02_train,Y02_train, label=\"Training data\")\n",
    "plt.scatter(X02_val,Y02_val, label=\"Expected values\")\n",
    "plt.plot(X02_val,Y02_pred, c=\"r\", marker=\"\", label=\"Linear regression\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Basis functions\n",
    "\n",
    "#### 3.1 Linear regression with polynomial basis functions\n",
    "\n",
    "From now on we will apply a non-linear transformation to ``X``, store the transformed data in a matrix ``phi`` and only then perform linear regression. \n",
    "This is called a linear basis functions model. \n",
    "\n",
    "In the lecture (slide 18), there is an example with a polynomial basis function $phi(x) = (1, x, x^2)$\n",
    "\n",
    "Questions:\n",
    "\n",
    "1. Function ``phi_polynomial``:\n",
    "   1. complete this function taking the data ``X`` and a max degree ``d`` and returns ``phi`` containing the following matrix \n",
    "\\begin{equation}\n",
    "\\Phi = \n",
    "\\begin{bmatrix}\n",
    "1 & x_{1} & x_{1}^2 & \\cdots & x_{1}^d \\\\\n",
    "1 & x_{2} & x_{2}^2 & \\cdots & x_{2}^d \\\\\n",
    "\\vdots & \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "1 & x_{n} & x_{n}^2 & \\cdots & x_{n}^d\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{(n, d+1)}\n",
    "\\end{equation}\n",
    "1. For all ``d`` in ``d_list``:\n",
    "   1. Training:\n",
    "      1. Compute ``phi_train`` by calling  ``phi_polynomial`` on your training data\n",
    "      2. Compute ``W`` by calling ``fit_LR`` on your transformed data and your training set of  expected values\n",
    "   2. Evaluation:\n",
    "      1. Transform your validation data and store them in ``phi_val``\n",
    "      2. Compute your predictions ``Y02_pred`` given your transformed validation data and your weights\n",
    "      3. Look at how the mse was computed with the training data. Do the same with your validation data and store the mse in ``mse_val``\n",
    "2. Interpretation:\n",
    "   1. What do you obtain for ``d=0``?\n",
    "   2. What do you obtain for ``d=1``? Compare with the cell above.\n",
    "   3. By looking at the mse, which degree would you choose for your model?\n",
    "   4. Models with a degree between 5 and 20 seem to work similarly. In the situation when several models of different complexity have approximately the same performance, what is usually the preferred option?\n",
    "   5. For which values of ``d`` does the model seem to overfit?\n",
    "   6. For ``d>16`` the performance on training data is getting worse. According to you, what could cause that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------\n",
    "# Polynomial basis functions\n",
    "# ----------------------\n",
    "def phi_polynomial(X,d):\n",
    "    \"\"\"\n",
    "    Compute polynomial basis functions\n",
    "    \n",
    "    The biais is included in this function\n",
    "    \"\"\"\n",
    "    # TODO!\n",
    "    return phi\n",
    "\n",
    "print(\" ============ Linear regression with polynomial basis functions ============\")\n",
    "mse_train = []\n",
    "mse_val = []\n",
    "d_list = list(range(6))+list(range(6,35,5))\n",
    "for d in d_list:\n",
    "    # ----------------------\n",
    "    # Training\n",
    "    # ----------------------\n",
    "    phi_train = phi_polynomial(..., ...)      # TODO!\n",
    "    # Note: The biais is already included in phi\n",
    "    W = fit_LR(..., ..., add_biais=False)     # TODO!\n",
    "    # ----------------------\n",
    "    # Evaluation\n",
    "    # ----------------------\n",
    "    # Evaluate performance on training data\n",
    "    # Note: The biais is already included in phi\n",
    "    Y02_pred_train = predict_LR(phi_train,W, add_biais=False)\n",
    "    mse_train.append(mean_squared_error(Y02_train, Y02_pred_train))\n",
    "    \n",
    "    # Evaluate performance on validation data\n",
    "    phi_val = phi_polynomial(..., ...)                   # TODO!\n",
    "    # Note: The biais is already included in phi\n",
    "    Y02_pred = predict_LR(... , ... , add_biais=False)   # TODO!\n",
    "    mse_val.append(...)                                  # TODO!\n",
    "    # ----------------------\n",
    "    # Plots\n",
    "    # ----------------------\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.title(\"Linear regression with polynomial basis functions \\ndegree d=\"+str(d))\n",
    "    plt.scatter(X02_train,Y02_train, label=\"Training data\")\n",
    "    plt.scatter(X02_val,Y02_val, label=\"Expected values\")\n",
    "    plt.plot(X02_val,Y02_pred, c=\"r\", marker=\"\", label=\"Predicted values\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# ----------------------\n",
    "# Overview\n",
    "# ----------------------\n",
    "def plot_mse(x_axis, mse_train, mse_val):\n",
    "    \"\"\"\n",
    "    Plot training and validation Mean Squared Error\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.ylim(min(min(mse_train),min(mse_val)), min(max(max(mse_train), max(mse_val)), 300))\n",
    "    plt.plot(x_axis, mse_train, label=\"Training\")\n",
    "    plt.plot(x_axis, mse_val, label=\"Validation\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "plot_mse(d_list, mse_train, mse_val)\n",
    "plt.title(\"MSE of linear regression with polynomial basis functions\")\n",
    "plt.xlabel(\"Max degree of the polynomial used as basis functions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Linear regression with polynomial basis functions on non-polynomial data\n",
    "\n",
    "1. Datasets \n",
    "   1. Split ``X03`` and ``Y03`` into 3 datasets, a training one, a validation one and a testing one with a ratio 0.6, 0.2, 0.2.\n",
    "2. Polynomial basis functions using scikit-learn:\n",
    "   1. Instantiate an object of the class [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn-linear-model-linearregression) from the scikit-learn library\n",
    "   2. Train your model using your training dataset.\n",
    "   3. Predict values using your model using your training dataset and then your validation dataset.\n",
    "   4. Compute the MSE of your predictions on your validation data and store the successive values in ``mse_val``\n",
    "   5. Plot training datapoints, expected datapoints from the validation dataset and your predictions\n",
    "   6. Plot the MSE as a function of the max degree of the polynomial used as basis function\n",
    "3. Interpretation:\n",
    "   1. Does the model work well?\n",
    "   2. Do you think polynomial functions are adapted to this problem?\n",
    "   3. For ``d>20`` the performance on training data is getting worse. According to you, what could cause that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" ============ Split datasets ============\")\n",
    "\n",
    "seed = 666                    # Fix random seed for reproducibility\n",
    "# Shuffle and split the data into train and a concatenation of validation and test sets\n",
    "... #TODO!\n",
    "seed = 221\n",
    "# Shuffle and split the data into validation and test sets\n",
    "... #TODO!  \n",
    "\n",
    "# sort with respect to x_axis for better plots\n",
    "... #TODO!\n",
    "\n",
    "# Show number of datapoints\n",
    "... #TODO!\n",
    "\n",
    "print(\" ============ Using scikit learn ============\")\n",
    "mse_train = []\n",
    "mse_val = []\n",
    "d_list = list(range(2,10))+list(range(10,30,5))\n",
    "for d in d_list:\n",
    "    poly_model = make_pipeline(PolynomialFeatures(d),\n",
    "                               LinearRegression())\n",
    "    # ----------------------\n",
    "    # Training\n",
    "    # ----------------------\n",
    "    # Use array.reshape(-1, 1) if your data has a single feature\n",
    "    poly_model.fit(... , ...) # TODO!\n",
    "    # ----------------------\n",
    "    # Evaluation\n",
    "    # ----------------------\n",
    "    # Evaluate performance on training data\n",
    "    # Use array.reshape(-1, 1) if your data has a single feature\n",
    "    y_pred_train = poly_model.predict(...)   # TODO!\n",
    "    mse_val.append(...)                      # TODO!\n",
    "    \n",
    "    # Evaluate performance on validation data\n",
    "    y_pred_scikit = poly_model.predict(...)  # TODO!\n",
    "    mse_val.append(...)                      # TODO!\n",
    "    # ----------------------\n",
    "    # Plots\n",
    "    # ----------------------\n",
    "    ... #TODO!\n",
    "    \n",
    "# ----------------------\n",
    "# Overview\n",
    "# ----------------------\n",
    "... #TODO!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression with cosine basis functions\n",
    "\n",
    "Since the polynomial basis functions did not seem to work well on these data, we will use another type of basis function: cosine functions. To do so, we need to implement a new function ``phi_cosine`` that will transform our data ``X`` using cosine functions.\n",
    "\n",
    "Questions:\n",
    "\n",
    "1. Function ``phi_cosine``:\n",
    "   1. Using [np.cos()](https://numpy.org/doc/stable/reference/generated/numpy.cos.html#numpy-cos), complete this function taking the data ``X`` and a number of cosine ``n_cosine`` and returns ``phi`` containing the following matrix \n",
    "\\begin{equation}\n",
    "\\Phi = \n",
    "\\begin{bmatrix}\n",
    "1 & cos(x_{1}) & cos(2   x_{1}) & \\cdots & cos(n_{cosine}   x_{1}) \\\\\n",
    "1 & cos(x_{2}) & cos(2   x_{2}) & \\cdots & cos(n_{cosine}   x_{2}) \\\\\n",
    "\\vdots & \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "1 & cos(x_{n}) & cos(2   x_{n}) & \\cdots & cos(n_{cosine}   x_{n})\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{(n, n_{cosine}+1)}\n",
    "\\end{equation}\n",
    "1. Training\n",
    "   1. Transform and fit your model using ``phi_cosine`` and ``fit_LR`` (Hint: look at what you did with polynomial functions)\n",
    "2. Evaluation\n",
    "   1. Use ``predict_LR`` on your training and validation datasets. \n",
    "   2. Compute and store your mean square errors in ``mse_train, mse_val``\n",
    "3. Interpretation\n",
    "   1. Does a cosine transformation seem to work better than a polynomial transformation on this data? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi_cosine(X,n_cosine):\n",
    "    # TODO!\n",
    "    return phi\n",
    "\n",
    "mse_train = []\n",
    "mse_val = []\n",
    "n_list = list(range(1,10))\n",
    "for n in n_list:\n",
    "    # ----------------------\n",
    "    # Training\n",
    "    # ----------------------\n",
    "    phi_train = phi_cosine(...)   # TODO!\n",
    "    W = fit_LR(..., ..., ...)     # TODO!\n",
    "    # ----------------------\n",
    "    # Evaluation\n",
    "    # ----------------------\n",
    "    # Evaluate performance on training data\n",
    "    Y03_pred_train = predict_LR(..., ..., ...)  # TODO!\n",
    "    mse_train.append(...)                       # TODO!\n",
    "    \n",
    "    # Evaluate performance on validation data\n",
    "    phi_val = phi_cosine(...)                   # TODO!\n",
    "    Y03_pred = predict_LR(..., ..., ...)        # TODO!\n",
    "    mse_val.append(...)                         # TODO!\n",
    "    # ----------------------\n",
    "    # Plots\n",
    "    # ----------------------\n",
    "    ... #TODO!\n",
    "    \n",
    "# ----------------------\n",
    "# Overview\n",
    "# ----------------------\n",
    "... #TODO!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Linear regression with gaussian basis functions\n",
    "\n",
    "\n",
    "In the lecture (slide 19), there is an example with a gaussian basis function $$\\Phi(x) = (e^{\\epsilon(||\\mathbf{x}-\\mathbf{c}||)})$$\n",
    "\n",
    "Where $\\mathbf{c}$ is the center points\n",
    "\n",
    "In the cell below we implemented ``phi_gaussian`` that works exactly as ``phi_polynomial`` and ``phi_cosine`` work.\n",
    "\n",
    "Questions:\n",
    "\n",
    "There are no questions for this one :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi_gaussian(X,n_gaussians, sigma=None):\n",
    "    \"\"\"\n",
    "    Gaussian basis functions, uniformly spaced\n",
    "    \n",
    "    The biais is included in this function\n",
    "    \"\"\"\n",
    "    phi = np.ones((len(X),n_gaussians+1))\n",
    "    centers = np.linspace(X.min(), X.max(), n_gaussians)\n",
    "    if sigma is None:\n",
    "        if n_gaussians < 2:\n",
    "            sigma = (X.max()-X.min())/2\n",
    "        else:\n",
    "            sigma = (centers[1]-centers[0])/2\n",
    "    for i in range(1,n_gaussians+1):\n",
    "        phi[:,i] = np.exp(-(X-centers[i-1])**2/sigma)\n",
    "    return phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Choose the right basis functions and the right parameters: model selection and evaluation\n",
    "\n",
    "In this notebook you could observe that the performance of a given transformation (basis function) depends a lot on your problem (i.e your data) \n",
    "\n",
    "Then once the transformation is chosen, the complexity of your model (how many basis functions you have) influences the performance too. \n",
    "\n",
    "In this cell you will use a new dataset: ``X04, Y04``. You will need to split it as we did for the 3 other datasets and you will have to choose only one model (i.e one type of basis function and the number of basis functions).\n",
    "\n",
    "You can choose among:\n",
    "\n",
    "- simple linear regression (no transformation)\n",
    "- polynomial basis functions\n",
    "- cosine basis functions\n",
    "- gaussian basis functions\n",
    "\n",
    "We encourage you to try them all.\n",
    "\n",
    "Then, for each type of transformation, tune your model parameters (which are ``d``, ``n_cosine``, ``n_gaussians`` here)\n",
    "\n",
    "Finally select (automatically, not manually) the best model based on its validation error. You can store all the validation errors in a list, and then find the model with the lowest validation error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "... #TODO!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Model evaluation\n",
    "\n",
    "Evaluate the performance of your selected model by computing its MSE error on the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "... #TODO!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('nglm-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "71c2cb666ff353b4e7b5c350d66179fa0af5c84ce239ad9fa105d94543f3ad59"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
