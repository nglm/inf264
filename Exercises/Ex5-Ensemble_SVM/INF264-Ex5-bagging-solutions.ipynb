{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrap Aggregation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples:  208\n",
      "Number of features:  60\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset and store it as a pandas dataframe\n",
    "dataset = pd.read_csv('sonar.all-data',header=None)\n",
    "# Store features in a pandas dataframe X\n",
    "X = dataset.iloc[:,:-1]\n",
    "# Convert X into a numpy array\n",
    "X = X.to_numpy()\n",
    "\n",
    "# Store labels in a numpy array y\n",
    "y = dataset.iloc[:,-1].to_numpy()\n",
    "# Convert it into a array of boolean (True if 'M' and False otherwise)\n",
    "y = (y == 'M')\n",
    "# Convert it into a array of int (1 if 'True' and 0 otherwise)\n",
    "y = y.astype(int)\n",
    "\n",
    "print(\"Number of samples: \", X.shape[0])\n",
    "print('Number of features: ', X.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KFold_split(X, Y, k=5, test_ratio=0.2, seed=264):\n",
    "    \"\"\"\n",
    "    Split dataset into a test dataset and train/val kfolds\n",
    "    \"\"\"\n",
    "    # Extract test set from entire dataset\n",
    "    X_train_val, X_test, Y_train_val, Y_test = train_test_split(X, Y, test_size=test_ratio, shuffle=True, random_state=seed)\n",
    "    \n",
    "    # Create train/validation kfolds splitter\n",
    "    KFold_splitter = KFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "    X_train_folds = []\n",
    "    X_val_folds = []\n",
    "    Y_train_folds = []\n",
    "    Y_val_folds = []\n",
    "    \n",
    "    # Split train_val dataset into folds\n",
    "    for (kth_fold_train_idxs, kth_fold_val_idxs) in KFold_splitter.split(X_train_val, Y_train_val):\n",
    "        X_train_folds.append(X_train_val[kth_fold_train_idxs])\n",
    "        X_val_folds.append(X_train_val[kth_fold_val_idxs])\n",
    "        Y_train_folds.append(Y_train_val[kth_fold_train_idxs])\n",
    "        Y_val_folds.append(Y_train_val[kth_fold_val_idxs])\n",
    "        \n",
    "    print(\"Training dataset size:   \", len(X_train_folds[0]))\n",
    "    print(\"Validation dataset size: \", len(X_val_folds[0]))\n",
    "    print(\"Test dataset size:       \", len(X_test))\n",
    "    return X_train_folds, Y_train_folds, X_val_folds, Y_val_folds, X_test, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsample function\n",
    "\n",
    "\n",
    "You can use the function [random.choices()](https://docs.python.org/3/library/random.html#random.choices) to get a subsampling with replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size:    132\n",
      "Validation dataset size:  34\n",
      "Test dataset size:        42\n",
      "Ratio of unique element in the subsample:  0.6212121212121212\n"
     ]
    }
   ],
   "source": [
    "def bootstrap_sample(\n",
    "    X,\n",
    "    y,\n",
    "    n_samples=None   # number of samples in the subsampling\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a random subsample from the dataset with replacement\n",
    "    \"\"\"\n",
    "    if n_samples is None:\n",
    "        n_samples = len(X)\n",
    "    idx = random.choices(range(len(X)), k=n_samples)\n",
    "    return X[idx], y[idx]\n",
    "\n",
    "def count_ratio_unique_bootstrap_sample(X_sample):\n",
    "    return len(np.unique(X_sample, axis=0))/len(X_sample)\n",
    "\n",
    "X_train_folds, Y_train_folds, X_val_folds, Y_val_folds, X_test, Y_test = KFold_split(X, y)\n",
    "\n",
    "X_sample, y_sample = bootstrap_sample(X_train_folds[0],Y_train_folds[0])\n",
    "print(\"Ratio of unique element in the subsample: \", count_ratio_unique_bootstrap_sample(X_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging train and predict functions\n",
    "\n",
    "\n",
    "**Note**: in scikit-learn, all supervised estimators implement a ``fit(X, y)`` method and a ``predict(X)`` method with ``X`` being unlabeled observations and  ``y`` being labels. \n",
    "\n",
    "Therefore ``Classifier`` parameter can be any sklearn class implementing a supervised classifier.\n",
    "\n",
    "(See *The problem solved in supervised learning* section in the supervised learning tutorial from [sklearn documentation](https://scikit-learn.org/stable/tutorial/statistical_inference/supervised_learning.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagging_train(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    n_clfs,                                  # number of classifier\n",
    "    Classifier = DecisionTreeClassifier,     # Python class of classifier\n",
    "    clfs_args = {},                          # Specific python class of classifier's arguments\n",
    "):\n",
    "    \"\"\"\n",
    "    Bootstrap Aggregation training algorithm\n",
    "    \"\"\"\n",
    "    clfs = []\n",
    "    for i in range(n_clfs):\n",
    "        # -------------------------\n",
    "        # Train a new classifier\n",
    "        # -------------------------\n",
    "        # Take a subsample of X and Y (with replacement)\n",
    "        sample_X, sample_y = bootstrap_sample(X_train, y_train)\n",
    "        # Initialize a new Classifier object\n",
    "        clf = Classifier(**clfs_args)\n",
    "        # Train this new Classifier object\n",
    "        clf.fit(sample_X,sample_y)\n",
    "        # Append your trained classifier in your list of classifiers \n",
    "        clfs.append(clf)\n",
    "    # Return the list of trained classifiers composing the bagging classifier\n",
    "    return clfs\n",
    "\n",
    "\n",
    "def bagging_predict(\n",
    "    clfs,     # list of classifiers composing the bagging classifier\n",
    "    X\n",
    "):\n",
    "    \"\"\"\n",
    "    Bootstrap Aggregation predict algorithm\n",
    "    \"\"\"\n",
    "    y_pred= []\n",
    "    for row in X:\n",
    "        # Get a prediction of 'row' for each classifier trained\n",
    "        predictions = [clf.predict([row])[0] for clf in clfs]\n",
    "        # Get the most common prediction and append it to 'y_pred'\n",
    "        y_pred.append(max(predictions, key=predictions.count))\n",
    "    return(np.array(y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging pipeline using cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagging_CV_pipeline(\n",
    "    n_clfs,           # number of classifiers\n",
    "    Classifier,       # Python class of classifier\n",
    "    clfs_args = {},   # Classifier's hparams\n",
    "):\n",
    "    \"\"\"\n",
    "    Cross validation step of the machine learning pipeline for a bagging algorithm\n",
    "    \"\"\"\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    \n",
    "    # For each set of k-folds get the bagging classifier and its accuracy\n",
    "    for X_train_fold, X_val_fold, y_train_fold, y_val_fold in zip(\n",
    "        X_train_folds, X_val_folds, Y_train_folds, Y_val_folds\n",
    "        ):\n",
    "        \n",
    "        # 'clfs' are the classifiers associated with the current bagging classifier\n",
    "        clfs = bagging_train(X_train_fold, y_train_fold, n_clfs, Classifier, clfs_args=clfs_args)\n",
    "        y_pred = bagging_predict(clfs, X_val_fold)\n",
    "        \n",
    "        # Training scores of the current bagging classifier\n",
    "        y_pred = bagging_predict(clfs, X_train_fold)\n",
    "        train_accs.append(accuracy_score(y_train_fold, y_pred))\n",
    "        \n",
    "        # Validation scores of the current bagging classifier\n",
    "        y_pred = bagging_predict(clfs, X_val_fold)\n",
    "        val_accs.append(accuracy_score(y_val_fold, y_pred))\n",
    "        \n",
    "    # Return the mean scores \n",
    "    return np.mean(train_accs), np.mean(val_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training different bagging models on the sonar dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of trees: 1\n",
      "Mean training accuracy:     0.8810\n",
      "Mean validation accuracy:   0.6567\n",
      "\n",
      "Number of trees: 5\n",
      "Mean training accuracy:     0.9669\n",
      "Mean validation accuracy:   0.7595\n",
      "\n",
      "Number of trees: 10\n",
      "Mean training accuracy:     0.9864\n",
      "Mean validation accuracy:   0.7414\n",
      "\n",
      "Number of trees: 15\n",
      "Mean training accuracy:     0.9970\n",
      "Mean validation accuracy:   0.7893\n",
      "\n",
      "Number of trees: 20\n",
      "Mean training accuracy:     0.9985\n",
      "Mean validation accuracy:   0.7412\n",
      "\n",
      "Number of trees: 25\n",
      "Mean training accuracy:     1.0000\n",
      "Mean validation accuracy:   0.7704\n",
      "\n",
      "Number of trees: 30\n",
      "Mean training accuracy:     1.0000\n",
      "Mean validation accuracy:   0.7531\n"
     ]
    }
   ],
   "source": [
    "mean_train_accs = []\n",
    "mean_val_accs = []\n",
    "list_n_trees = [1, 5, 10, 15, 20, 25, 30]\n",
    "\n",
    "# For each hyper-parameter instance, do KFold cross validation:\n",
    "for n_trees in list_n_trees:\n",
    "    mean_train_acc, mean_val_acc = bagging_CV_pipeline(\n",
    "        n_clfs = n_trees,\n",
    "        Classifier = DecisionTreeClassifier,\n",
    "        clfs_args={}\n",
    "    )\n",
    "    print('\\nNumber of trees: %d' % n_trees)\n",
    "    print('Mean training accuracy:     %.4f' %mean_train_acc)\n",
    "    print('Mean validation accuracy:   %.4f' %mean_val_acc)\n",
    "    \n",
    "    mean_train_accs.append(mean_train_acc)\n",
    "    mean_val_accs.append(mean_val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting and evaluating the best bagging model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best model selected with n_trees= 15\n",
      "\n",
      "Training accuracy: (incl. validation dataset)    1.0\n",
      "Test accuracy:                                   0.7380952380952381\n"
     ]
    }
   ],
   "source": [
    "# Select best model\n",
    "i_best = np.argmax(mean_val_accs)\n",
    "\n",
    "# Evaluate best model\n",
    "X_train_val = np.concatenate([X_train_folds[0], X_val_folds[0]])\n",
    "Y_train_val = np.concatenate([Y_train_folds[0], Y_val_folds[0]])\n",
    "best_model = bagging_train(X_train_val, Y_train_val, list_n_trees[i_best])\n",
    "\n",
    "print(\"\\nBest model selected with n_trees=\", list_n_trees[i_best])\n",
    "y_pred = bagging_predict(best_model, X_train_val)\n",
    "acc = accuracy_score(Y_train_val, y_pred)\n",
    "print(\"\\nTraining accuracy: (incl. validation dataset)   \", acc)\n",
    "y_pred = bagging_predict(best_model, X_test)\n",
    "acc = accuracy_score(Y_test, y_pred)\n",
    "print(\"Test accuracy:                                  \", acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('nglm-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "71c2cb666ff353b4e7b5c350d66179fa0af5c84ce239ad9fa105d94543f3ad59"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
