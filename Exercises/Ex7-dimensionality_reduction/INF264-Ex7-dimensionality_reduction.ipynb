{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection\n",
    "\n",
    "## Contents:\n",
    "\n",
    "1. Pairplot visualization and domain knowlegde \n",
    "2. Filter methods\n",
    "3. Wrapper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectFromModel, mutual_info_regression, f_regression\n",
    "from scipy.stats import pearsonr, spearmanr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pairplot visualization and domain knowlegde \n",
    "\n",
    "### Main Objectives:\n",
    "\n",
    "Remove:\n",
    "- Constant or almost constant features\n",
    "- Obviously irrelevant features with respect to the target\n",
    "- Obviously redundant features\n",
    "\n",
    "It is also always nice to have a visual idea of the relationships between features and between each feature and the target\n",
    "\n",
    "### Pairplot visualization\n",
    "\n",
    "Here are toy examples of pairplots. Each of these plots are independant.\n",
    "\n",
    "![Toy example](figs/toy_ex.png)\n",
    "\n",
    "\n",
    "#### QUESTIONS\n",
    "\n",
    "For each of the following figures (ignore statistics related to each plot for now):\n",
    "\n",
    "  1. Interpret the figure assuming that the x axis is a feature and the y axis the target. Would you keep the feature? \n",
    "  2. Interpret the figure assuming that both x and y axis are features. Would you keep both features? \n",
    "\n",
    "\n",
    "### Domain knowlegde\n",
    "\n",
    "![Boston extended dataframe](figs/boston_preview.png)\n",
    "\n",
    "Attribute Information (in order):\n",
    "  - CRIM     $\\quad$ per capita crime rate by town\n",
    "  - ZN       $\\quad$ proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "  - INDUS    $\\quad$ proportion of non-retail business acres per town\n",
    "  - NOX      $\\quad$ nitric oxides concentration (parts per 10 million)\n",
    "  - RM       $\\quad$ average number of rooms per dwelling\n",
    "  - OLD      $\\quad$ proportion of owner-occupied units built prior to 1940\n",
    "  - NEW      $\\quad$ proportion of owner-occupied units built after 1940\n",
    "  - DIS      $\\quad$ weighted distances to five Boston employment centres\n",
    "  - RAD      $\\quad$ index of accessibility to radial highways\n",
    "  - TAX      $\\quad$ full-value property-tax rate per $10,000\n",
    "  - PTRATIO  $\\quad$ pupil-teacher ratio by town\n",
    "  - B        $\\quad$ 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "  - Bk       $\\quad$ Proportion of blacks by town\n",
    "  - LSTAT    $\\quad$ % lower status of the population\n",
    "  - MEDV     $\\quad$ Median value of owner-occupied homes in $1000's\n",
    "\n",
    "#### QUESTIONS\n",
    "\n",
    "1. Read feature descriptions. Choose 2 features that could probably be removed. \n",
    "\n",
    "\n",
    "### Limits of pairplot visualization and domain knowlegde \n",
    "- What if 1000 features? The process is not automated...\n",
    "- Subjective choice\n",
    "- Only obvious things can be spotted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Filter methods\n",
    "\n",
    "Filter methods select the most relevant features based on statistical measures of correlation or dependence between variables.\n",
    "\n",
    "But... Which statistical tests to use? There are many tests and they are not always easy to interpret and we advise against using measures you do not understand. We will illustrate this fact with 3 statistical tests: Pearson and Spearman correlations and mutual information.\n",
    "\n",
    "#### QUESTIONS\n",
    "\n",
    "1. Read the short description of the python functions listed below and interpret their corresponding output given in the toy examples pairplots of section 1. \n",
    "2. For each of the 3 tests, give at least one disadvantage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pearson correlation: linear relationship\n",
    "\n",
    "\n",
    "[pearsonr()](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html) or [df.corr(method='pearson')](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html)\n",
    "\n",
    "The Pearson correlation coefficient measures the linear relationship between two datasets. Like other correlation coefficients, this one varies between -1 and +1 with 0 implying no correlation. Correlations of -1 or +1 imply an exact linear relationship. Positive correlations imply that as x increases, so does y. Negative correlations imply that as x increases, y decreases.\n",
    "\n",
    "The calculation of the p-value relies on the assumption that each dataset is normally distributed. The p-value roughly indicates the probability of an uncorrelated system producing datasets that have a Pearson correlation at least as extreme as the one computed from these datasets.\n",
    "\n",
    "#### Spearman correlation: monotonic relationship\n",
    "\n",
    "\n",
    "[spearmanr()](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html) or [df.corr(method='spearman')](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html)\n",
    "\n",
    "\n",
    "The Spearman rank-order correlation coefficient is a nonparametric measure of the monotonicity of the relationship between two datasets. Like other correlation coefficients, this one varies between -1 and +1 with 0 implying no correlation. Correlations of -1 or +1 imply an exact monotonic relationship. Positive correlations imply that as x increases, so does y. Negative correlations imply that as x increases, y decreases.\n",
    "\n",
    "The p-value roughly indicates the probability of an uncorrelated system producing datasets that have a Spearman correlation at least as extreme as the one computed from these datasets. Unlike the Pearson correlation, the Spearman correlation does not assume that both datasets are normally distributed. The p-values are not entirely reliable but are probably reasonable for datasets larger than 500 or so.\n",
    "\n",
    "#### Mutual information \n",
    "\n",
    "[mutual_info_regression()](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html)\n",
    "\n",
    "Mutual information (MI) between two random variables is a non-negative value, which measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency. This measure is NOT upper bounded as pearson and spearman coefficients are!\n",
    "\n",
    "The function relies on nonparametric methods based on entropy estimation from k-nearest neighbors distances. It can be used for univariate features selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On the Boston dataset\n",
    "\n",
    "We have extended the orginal Boston dataset and stored it into the ``boston_extended.csv`` file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('boston_extended.csv',index_col=0)  # Load the dataset\n",
    "cols = df.columns.tolist()\n",
    "X = df[cols[:-1]]     # Store features in X\n",
    "y = df[cols[-1]]      # Store target in y\n",
    "\n",
    "# Split the datasets\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X,y, shuffle=True, test_size=0.2, random_state=12)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, shuffle=True, test_size=0.5, random_state=3)\n",
    "\n",
    "print('Training size:   ', len(y_train))\n",
    "print('Validation size: ', len(y_val))\n",
    "print('Test size:       ', len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO DO**: \n",
    "\n",
    "1. Use [sns.pairplot](https://seaborn.pydata.org/generated/seaborn.pairplot.html) to visualize the different relationships between the features and between each feature and the target (**WARNING** this might take a while. If your computer seems a bit weak don't insist)\n",
    "\n",
    "**QUESTIONS**\n",
    "\n",
    "1. Look at the pairplots of the features you would have removed in section 1. Do you confirm that you can remove these features? \n",
    "2. Are there other obvious features you can remove? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "... #TODO!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Wrapper methods\n",
    "\n",
    "Unlike filter methods, wrapper methods make use of a learning algorithm to select features. Instead of measuring the importance of each feature with statistical tests, it compares the machine learning performance of a given model trained with different features to select the best ones.  \n",
    "\n",
    "### Forward selection\n",
    "\n",
    "The objective of forward selection is to iteratively select the best features with respect to a base estimator starting from an empty set of features. In other words, at each iteration it selects the feature that maximizes the performance of a given model and add it to the set of selected feature. \n",
    "\n",
    "**TODO:** Write a function that implements the forward selection algorithm described below, using a linear regression model and the r2 score on the training dataset. (Read the [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) documentation to find out how to get the r2 score)\n",
    "\n",
    "\n",
    "  Input: ``data``, ``target``,``max_nb_features``\n",
    "  \n",
    "  0. Start with an empty list of ``best_features``, and a list of ``remaining_features`` initialized to the entire original   set of features\n",
    "  1. While there are still remaining features and while you have fewer than ``nb_max_features`` features selected:\n",
    "      1. Train a model using your target and your current best features with one extra feature from the remaining features   list\n",
    "      2. Select the extra feature that maximizes the performance of the model.\n",
    "      3. Add the selected feature to your best_features list and remove it from your remaining_features list\n",
    "\n",
    "\n",
    "**TODO:** Write a function that implements the same algorithm but with a threshold on the score instead of a predefined max number of selected features (replace then the ``max_nb_features`` parameter with a ``threshold`` parameter)\n",
    "\n",
    "**TODO:** Select the best $5$ features using ``forward_selection_max_nb_features`` on the training Boston dataset\n",
    "\n",
    "**TODO:** Select the best features using ``forward_selection_threshold`` on the training Boston dataset with a threshold of $0.65$ then $0.7$ and $0.75$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------\n",
    "# Using a predefined max number of features\n",
    "#-----------------------------------------------\n",
    "\n",
    "def forward_selection_max_nb_features(data, target, max_nb_features):\n",
    "    ... #TODO!\n",
    "    return best_features\n",
    "\n",
    "\n",
    "#-----------------------------------------------\n",
    "# Using a threshold\n",
    "#-----------------------------------------------\n",
    "\n",
    "def forward_selection_threshold(data, target, threshold):\n",
    "    ... #TODO!\n",
    "    return best_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using sklearn 'SelectFromModel'\n",
    "\n",
    "**TODO:** Use the [feature_selection.SelectFromModel()](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html) sklearn class to select a predefined number of features (try $2, 3$ and $5$) using a LinearRegression estimator. Use it on the training Boston dataset \n",
    "\n",
    "**TODO:** Use the [feature_selection.SelectFromModel()](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html) sklearn class to select features whose score are above thea given threshold using a LinearRegression estimator. Use it on the training Boston dataset \n",
    "\n",
    "**QUESTION**\n",
    "\n",
    "What is the score implicitly used to select the features here?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------\n",
    "# Using a predefined max number of features\n",
    "#-----------------------------------------------\n",
    "... #TODO!\n",
    "\n",
    "#-----------------------------------------------\n",
    "# Using a threshold\n",
    "#-----------------------------------------------\n",
    "... #TODO!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('graphapp-CGLyoVIh-py3.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "cd17699b0f8c8ba78222bc090e4903241cbfccfe2c3d9b347e7b5dfb6ba74475"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
