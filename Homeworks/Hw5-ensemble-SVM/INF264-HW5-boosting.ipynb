{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import log, exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load and prepare data\n",
    "dataset = pd.read_csv('sonar.all-data',header=None)\n",
    "X = dataset.iloc[:,:-1].to_numpy()\n",
    "# Labels should be -1 and 1!\n",
    "y = (dataset.iloc[:,-1].to_numpy()=='M').astype(int)\n",
    "y = np.where(y==0, -np.ones_like(y), y)\n",
    "print(\"Number of samples:  \", X.shape[0])\n",
    "print('Number of features: ', X.shape[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting train and predict functions\n",
    "\n",
    "\n",
    "**Note 1**: \n",
    "\n",
    "in scikit-learn, all supervised estimators implement a ``fit(X, y)`` method and a ``predict(X)`` method with ``X`` being unlabeled observations and  ``y`` being labels. \n",
    "\n",
    "Therefore ``Classifier`` parameter can be any sklearn class implementing a supervised classifier.\n",
    "\n",
    "(See *The problem solved in supervised learning* section in the supervised learning tutorial from [sklearn documentation](https://scikit-learn.org/stable/tutorial/statistical_inference/supervised_learning.html)\n",
    "\n",
    "**Note 2**: \n",
    "\n",
    "Some sklearn classifiers (such as [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html), [SVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html), [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html), etc.)  have a ``sample_weight`` parameters in their ``fit`` and ``score`` methods, making it easy to implement a user-defined boosting algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boosting_train(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    n_clfs,           # number of classifier\n",
    "    Classifier,       # Python class of classifier\n",
    "    clfs_args = {},   # Specific python class of classifier's arguments\n",
    "):\n",
    "    \"\"\"\n",
    "    Adaboost training lgorithm\n",
    "    \"\"\"\n",
    "    clfs = []\n",
    "    alphas = []\n",
    "    # Initialize weights to 1/n\n",
    "    #TODO!\n",
    "    for t in range(n_clfs):\n",
    "        # -------------------------\n",
    "        # Train a new classifier\n",
    "        # -------------------------\n",
    "\n",
    "        # Train a weak learner using the training data and the sample weights\n",
    "        #TODO!\n",
    "        # Compute weighted training accuracy\n",
    "        #TODO!\n",
    "        # Compute weighted training error \n",
    "        #TODO!\n",
    "        # Compute alpha_t (and avoid math errors)\n",
    "        #TODO!\n",
    "\n",
    "        # -------------------------\n",
    "        # Update weights\n",
    "        # -------------------------\n",
    "        # Weights increase only if y_train != y_pred \n",
    "        # so that we concentrate on “hardest” examples\n",
    "        #TODO!\n",
    "        # Normalize weights\n",
    "        #TODO!\n",
    "    # Return the list of trained classifiers composing the boosting classifier\n",
    "    # with their corresponding weights 'alphas'\n",
    "    return(clfs, alphas)\n",
    "\n",
    "def boosting_predict(\n",
    "    clfs,       # list of classifiers composing the boosting classifier\n",
    "    alphas,     # Weights associated with each classifier in 'clfs'\n",
    "    X,\n",
    "):\n",
    "    \"\"\"\n",
    "    Adaboost predict algorithm\n",
    "    \"\"\"\n",
    "    #TODO!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting pipeline using cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training different boosting models on the sonar dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#TODO!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting and evaluating the best boosting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('nglm-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "71c2cb666ff353b4e7b5c350d66179fa0af5c84ce239ad9fa105d94543f3ad59"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
