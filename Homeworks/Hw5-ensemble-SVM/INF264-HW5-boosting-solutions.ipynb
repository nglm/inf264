{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import log, exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples:   208\n",
      "Number of features:  60\n"
     ]
    }
   ],
   "source": [
    "# load and prepare data\n",
    "dataset = pd.read_csv('sonar.all-data',header=None)\n",
    "X = dataset.iloc[:,:-1].to_numpy()\n",
    "# Labels should be -1 and 1!\n",
    "y = (dataset.iloc[:,-1].to_numpy()=='M').astype(int)\n",
    "y = np.where(y==0, -np.ones_like(y), y)\n",
    "print(\"Number of samples:  \", X.shape[0])\n",
    "print('Number of features: ', X.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size:    132\n",
      "Validation dataset size:  34\n",
      "Test dataset size:        42\n"
     ]
    }
   ],
   "source": [
    "def KFold_split(X, Y, k=5, test_ratio=0.2, seed=264):\n",
    "    \"\"\"\n",
    "    Split dataset into a test dataset and train/val kfolds\n",
    "    \"\"\"\n",
    "    # Extract test set from entire dataset\n",
    "    X_train_val, X_test, Y_train_val, Y_test = train_test_split(X, Y, test_size=test_ratio, shuffle=True, random_state=seed)\n",
    "    \n",
    "    # Create train/validation kfolds splitter\n",
    "    KFold_splitter = KFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "    X_train_folds = []\n",
    "    X_val_folds = []\n",
    "    Y_train_folds = []\n",
    "    Y_val_folds = []\n",
    "    \n",
    "    # Split train_val dataset into folds\n",
    "    for (kth_fold_train_idxs, kth_fold_val_idxs) in KFold_splitter.split(X_train_val, Y_train_val):\n",
    "        X_train_folds.append(X_train_val[kth_fold_train_idxs])\n",
    "        X_val_folds.append(X_train_val[kth_fold_val_idxs])\n",
    "        Y_train_folds.append(Y_train_val[kth_fold_train_idxs])\n",
    "        Y_val_folds.append(Y_train_val[kth_fold_val_idxs])\n",
    "        \n",
    "    print(\"Training dataset size:   \", len(X_train_folds[0]))\n",
    "    print(\"Validation dataset size: \", len(X_val_folds[0]))\n",
    "    print(\"Test dataset size:       \", len(X_test))\n",
    "    return X_train_folds, Y_train_folds, X_val_folds, Y_val_folds, X_test, Y_test\n",
    "\n",
    "X_train_folds, Y_train_folds, X_val_folds, Y_val_folds, X_test, Y_test = KFold_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting train and predict functions\n",
    "\n",
    "\n",
    "**Note 1**: \n",
    "\n",
    "in scikit-learn, all supervised estimators implement a ``fit(X, y)`` method and a ``predict(X)`` method with ``X`` being unlabeled observations and  ``y`` being labels. \n",
    "\n",
    "Therefore ``Classifier`` parameter can be any sklearn class implementing a supervised classifier.\n",
    "\n",
    "(See *The problem solved in supervised learning* section in the supervised learning tutorial from [sklearn documentation](https://scikit-learn.org/stable/tutorial/statistical_inference/supervised_learning.html)\n",
    "\n",
    "**Note 2**: \n",
    "\n",
    "Some sklearn classifiers (such as [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html), [SVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html), [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html), etc.)  have a ``sample_weight`` parameters in their ``fit`` and ``score`` methods, making it easy to implement a user-defined boosting algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boosting_train(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    n_clfs,           # number of classifier\n",
    "    Classifier,       # Python class of classifier\n",
    "    clfs_args = {},   # Specific python class of classifier's arguments\n",
    "):\n",
    "    \"\"\"\n",
    "    Adaboost training lgorithm\n",
    "    \"\"\"\n",
    "    clfs = []\n",
    "    alphas = []\n",
    "\n",
    "    n = len(X_train) \n",
    "    # Initialize weights to 1/n\n",
    "    sample_weight = np.ones((n,))/n\n",
    "    for t in range(n_clfs):\n",
    "        # -------------------------\n",
    "        # Train a new classifier\n",
    "        # -------------------------\n",
    "\n",
    "        # Train a weak learner using the training data and the sample weights\n",
    "        clf = Classifier(**clfs_args)\n",
    "        clf.fit(X_train, y_train, sample_weight=sample_weight)\n",
    "        clfs.append(clf)\n",
    "        # Compute weighted training accuracy\n",
    "        weighted_acc = clf.score(X_train, y_train, sample_weight=sample_weight)\n",
    "        # Compute weighted training error \n",
    "        weighted_err = 1 - weighted_acc\n",
    "        # Compute alpha_t (and avoid math errors)\n",
    "        if weighted_err == 0:\n",
    "            weighted_err = exp(-10)\n",
    "        elif weighted_err == 1:\n",
    "            weighted_err = 1-exp(-10)\n",
    "        alpha = 1/2 * log((1-weighted_err)/weighted_err)\n",
    "        alphas.append(alpha)\n",
    "\n",
    "        # -------------------------\n",
    "        # Update weights\n",
    "        # -------------------------\n",
    "        y_pred = clf.predict(X_train)\n",
    "        # Weights increase only if y_train != y_pred \n",
    "        # so that we concentrate on “hardest” examples\n",
    "        sample_weight = sample_weight * np.exp(-alpha * y_train * y_pred)\n",
    "        # Normalize weights\n",
    "        sample_weight /= np.linalg.norm(sample_weight)\n",
    "    # Return the list of trained classifiers composing the boosting classifier\n",
    "    # with their corresponding weights 'alphas'\n",
    "    return(clfs, alphas)\n",
    "\n",
    "def boosting_predict(\n",
    "    clfs,       # list of classifiers composing the boosting classifier\n",
    "    alphas,     # Weights associated with each classifier in 'clfs'\n",
    "    X,\n",
    "):\n",
    "    \"\"\"\n",
    "    Adaboost predict algorithm\n",
    "    \"\"\"\n",
    "    y_pred= []\n",
    "    for row in X:\n",
    "        # Get a prediction of 'row' for each classifier trained\n",
    "        predictions = [clf.predict([row])[0] for clf in clfs]\n",
    "        # Get the adaboost prediction by taking the (weighted) majority vote\n",
    "        adaboost_pred = np.sign(np.sum(np.array(alphas)*np.array(predictions)))\n",
    "        y_pred.append(adaboost_pred)\n",
    "    return(np.array(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting pipeline using cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boosting_CV_pipeline(\n",
    "    n_clfs,                              # number of classifiers\n",
    "    Classifier = DecisionTreeClassifier, # Python class of classifier\n",
    "    clfs_args = {\"max_depth\" : 1},       # Classifier's hparams\n",
    "):\n",
    "    \"\"\"\n",
    "    Cross validation step of the machine learning pipeline for a boosting algorithm\n",
    "    \"\"\"\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    \n",
    "    # For each set of k-folds get the bagging classifier and its accuracy\n",
    "    for X_train_fold, X_val_fold, y_train_fold, y_val_fold in zip(\n",
    "        X_train_folds, X_val_folds, Y_train_folds, Y_val_folds\n",
    "        ):\n",
    "        \n",
    "        # 'clfs' are the classifiers associated with the current boosting classifier\n",
    "        clfs, alphas = boosting_train(\n",
    "            X_train_fold, \n",
    "            y_train_fold, \n",
    "            n_clfs, \n",
    "            Classifier = Classifier, \n",
    "            clfs_args = clfs_args,\n",
    "        )\n",
    "        \n",
    "        # Training scores of the current bagging classifier\n",
    "        y_pred = boosting_predict(clfs, alphas, X_train_fold)\n",
    "        train_accs.append(accuracy_score(y_train_fold, y_pred))\n",
    "        \n",
    "        # Validation scores of the current bagging classifier\n",
    "        y_pred = boosting_predict(clfs, alphas, X_val_fold)\n",
    "        val_accs.append(accuracy_score(y_val_fold, y_pred))\n",
    "        \n",
    "    # Return the mean scores \n",
    "    return np.mean(train_accs), np.mean(val_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training different boosting models on the sonar dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of trees: 1\n",
      "Mean training accuracy:     0.7635\n",
      "Mean validation accuracy:   0.6868\n",
      "\n",
      "Number of trees: 10\n",
      "Mean training accuracy:     0.9458\n",
      "Mean validation accuracy:   0.7471\n",
      "\n",
      "Number of trees: 50\n",
      "Mean training accuracy:     1.0000\n",
      "Mean validation accuracy:   0.8435\n",
      "\n",
      "Number of trees: 100\n",
      "Mean training accuracy:     1.0000\n",
      "Mean validation accuracy:   0.8496\n",
      "\n",
      "Number of trees: 200\n",
      "Mean training accuracy:     1.0000\n",
      "Mean validation accuracy:   0.8556\n",
      "\n",
      "Number of trees: 300\n",
      "Mean training accuracy:     1.0000\n",
      "Mean validation accuracy:   0.8615\n",
      "\n",
      "Number of trees: 400\n",
      "Mean training accuracy:     1.0000\n",
      "Mean validation accuracy:   0.8615\n"
     ]
    }
   ],
   "source": [
    "Classifier = DecisionTreeClassifier\n",
    "clfs_args={\"max_depth\" : 1}\n",
    "\n",
    "mean_train_accs = []\n",
    "mean_val_accs = []\n",
    "list_n_clfs = [1, 10, 50, 100, 200, 300, 400]\n",
    "\n",
    "# For each hyper-parameter instance, do KFold cross validation:\n",
    "for n_clfs in list_n_clfs:\n",
    "    mean_train_acc, mean_val_acc =  boosting_CV_pipeline(\n",
    "        n_clfs = n_clfs,\n",
    "        Classifier = Classifier,\n",
    "        clfs_args = clfs_args\n",
    "    )\n",
    "    print('\\nNumber of trees: %d' % n_clfs)\n",
    "    print('Mean training accuracy:     %.4f' %mean_train_acc)\n",
    "    print('Mean validation accuracy:   %.4f' %mean_val_acc)\n",
    "\n",
    "    mean_train_accs.append(mean_train_acc)\n",
    "    mean_val_accs.append(mean_val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting and evaluating the best boosting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best model selected with n_trees= 300\n",
      "\n",
      "Training accuracy: (incl. validation dataset)    1.0\n",
      "Test accuracy:                                   0.8095238095238095\n"
     ]
    }
   ],
   "source": [
    "# Select best model\n",
    "i_best = np.argmax(mean_val_accs)\n",
    "\n",
    "# Evaluate best model\n",
    "X_train_val = np.concatenate([X_train_folds[0], X_val_folds[0]])\n",
    "Y_train_val = np.concatenate([Y_train_folds[0], Y_val_folds[0]])\n",
    "best_model, best_alphas = boosting_train(\n",
    "    X_train_val, Y_train_val, list_n_clfs[i_best], Classifier, clfs_args\n",
    ")\n",
    "\n",
    "print(\"\\nBest model selected with n_trees=\", list_n_clfs[i_best])\n",
    "y_pred = boosting_predict(best_model, best_alphas, X_train_val)\n",
    "acc = accuracy_score(Y_train_val, y_pred)\n",
    "print(\"\\nTraining accuracy: (incl. validation dataset)   \", acc)\n",
    "y_pred = boosting_predict(best_model, best_alphas, X_test)\n",
    "acc = accuracy_score(Y_test, y_pred)\n",
    "print(\"Test accuracy:                                  \", acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('nglm-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "71c2cb666ff353b4e7b5c350d66179fa0af5c84ce239ad9fa105d94543f3ad59"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
