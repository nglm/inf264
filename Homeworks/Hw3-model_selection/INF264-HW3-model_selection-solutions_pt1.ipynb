{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KFold cross-validation for regression and classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Model selection for regression\n",
    "\n",
    "### Question 1: load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Boston dataset:\n",
    "boston = load_boston()\n",
    "X = boston.data\n",
    "Y = boston.target\n",
    "n_samples, n_features = X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Investigate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplots of target prices with respect to each of the 13 features:\n",
    "fig, axes = plt.subplots(5,3, figsize=(15, 12), tight_layout=True)\n",
    "for i, ax in enumerate(axes.flat[:n_features]) :\n",
    "    ax.scatter(X[:,i], Y, marker='o')\n",
    "    ax.set_xlabel(boston.feature_names[i])\n",
    "    ax.set_ylabel('Target price')\n",
    "fig.suptitle(\"Target prices with respect to each of the 13 features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 Extract 2 relevant and continuous features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target prices appear to be strongly correlated to several continuous features... among them, features 5 (\"RM\") and 12 (\"LSTAT\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:,5], X[:,12])\n",
    "ax.set_title(\"Visualizing correlation of features 5 & 12\")\n",
    "ax.set_xlabel(boston.feature_names[5])\n",
    "ax.set_ylabel(boston.feature_names[12])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only features 5 and 12\n",
    "X = X[:,[5,12]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6: Simple machine learning pipeline (yes I do question 6 before 4 and 5 :)... )\n",
    "\n",
    "Here we define functions implementing the basic building blocks of a very simple machine learning pipeline as well as the pipeline itself, which includes:\n",
    "\n",
    "1. dataset split into 3 datasets training, validation and test\n",
    "2. training\n",
    "3. model selection\n",
    "4. model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(X, Y, test_ratio=0.2, val_ratio=0.2, seed=264):\n",
    "    \"\"\"\n",
    "    Split dataset into training, validation, test datasets\n",
    "    \"\"\"\n",
    "    # Extract test set from entire dataset\n",
    "    X_train_val, X_test, Y_train_val, Y_test = train_test_split(X, Y, test_size=test_ratio, shuffle=True, random_state=seed)\n",
    "    # Extract validation set from train_val dataset\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(X_train_val, Y_train_val, test_size=val_ratio, shuffle=True, random_state=seed)\n",
    "    \n",
    "    print(\"Training dataset size:   \", len(X_train))\n",
    "    print(\"Validation dataset size: \", len(X_val))\n",
    "    print(\"Test dataset size:       \", len(X_test))\n",
    "    return X_train, Y_train, X_val, Y_val, X_test, Y_test\n",
    "\n",
    "def polynomial_model(degree, regularization, X_train, X_val, X_test):\n",
    "    \"\"\"\n",
    "    Instantiate a Ridge model and transform data into polynomial features\n",
    "    \"\"\"\n",
    "    # Build the polynomial features:\n",
    "    poly_features = PolynomialFeatures(degree=degree)\n",
    "    X_train_poly = poly_features.fit_transform(X_train)\n",
    "    X_val_poly = poly_features.transform(X_val)\n",
    "    X_test_poly = poly_features.transform(X_test)\n",
    "    # Instantiate a Ridge model:\n",
    "    model = Ridge(alpha=regularization)\n",
    "    return model, X_train_poly, X_val_poly, X_test_poly\n",
    "\n",
    "def evaluate(model, X, Y, metric):\n",
    "    \"\"\"\n",
    "    Evaluate (already trained) model performance on the given dataset \n",
    "    \n",
    "    'metric' is performance metric, a python function comparing 'Y' and 'Y_pred'\n",
    "    \"\"\"\n",
    "    Y_pred = model.predict(X)\n",
    "    perf = metric(Y, Y_pred)\n",
    "    return perf\n",
    "    \n",
    "def model_selection(hparams, val_perf, minimize):\n",
    "    \"\"\"\n",
    "    Return index of the best model given their validation performances\n",
    "    \n",
    "    'hparams' and 'val_perf' have the same number of elements\n",
    "    \n",
    "    Note: Some performance metrics have to be minimized (e.g. MSE) and \n",
    "    some have to be maximized (e.g. accuracy, f1-score)\n",
    "    \"\"\"\n",
    "    if minimize:\n",
    "        i_best = np.argmin(val_perf)\n",
    "    else:\n",
    "        i_best = np.argmax(val_perf)\n",
    "    print(\"Best model selected, with\")\n",
    "    print(\"hyperparameters:        \", hparams[i_best])\n",
    "    return i_best\n",
    "    \n",
    "def model_evaluation(model, X_train, Y_train, X_val, Y_val, X_test, Y_test, metric):   \n",
    "    \"\"\"\n",
    "    Evaluate the selected model\n",
    "    \n",
    "    Train on the entire training/validation dataset\n",
    "    Evaluate performance on both training/validation dataset and test dataset\n",
    "    \"\"\"\n",
    "    # Take the whole training/validation dataset\n",
    "    X_train_val = np.concatenate((X_train, X_val))\n",
    "    Y_train_val = np.concatenate((Y_train, Y_val))\n",
    "    \n",
    "    # Train on the entire training/validation dataset\n",
    "    model.fit(X_train_val, Y_train_val)\n",
    "    \n",
    "    # Evaluate performance\n",
    "    train_val_perf = evaluate(model, X_train_val, Y_train_val, metric)\n",
    "    test_perf = evaluate(model, X_test, Y_test, metric)\n",
    "    print(\"Selected model performance:\")\n",
    "    print(\"Training (incl val):   %.4f\" %train_val_perf)\n",
    "    print(\"Test:                  %.4f\" %test_perf) \n",
    "    return model, train_val_perf, test_perf\n",
    "\n",
    "def basic_pipeline(\n",
    "    X, Y, hparams, performance, \n",
    "    test_ratio=0.2, val_ratio=0.2, seed=264\n",
    "):\n",
    "    \"\"\"\n",
    "    Typical simple machine learning pipeline\n",
    "    \"\"\"\n",
    "\n",
    "    # i. and ii.: Split dataset into a train, validation and test set\n",
    "    X_train, Y_train, X_val, Y_val, X_test, Y_test = split_dataset(\n",
    "        X, Y, test_ratio=test_ratio, val_ratio=val_ratio, seed=seed)\n",
    "    \n",
    "    train_perfs = []\n",
    "    val_perfs = []\n",
    "    models = []\n",
    "    # iii. Loop over sets of hyperparameters:\n",
    "    for hparam in hparams:\n",
    "        print(\"\\nUsing hyper-parameters:\", hparam)\n",
    "        \n",
    "        # Instantiate model with current set of hyperparameter\n",
    "        model, X_train_poly, X_val_poly, X_test_poly = polynomial_model(\n",
    "            hparam['degree'], hparam[\"regularization\"], X_train, X_val, X_test\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        model.fit(X_train_poly, Y_train)\n",
    "        \n",
    "        # Compute train and validation loss\n",
    "        train_perf = evaluate(model, X_train_poly, Y_train, performance[\"metric\"])\n",
    "        val_perf = evaluate(model, X_val_poly, Y_val, performance[\"metric\"])\n",
    "        print(\"Training MSE:    %.4f\" %train_perf)\n",
    "        print(\"Validation MSE:  %.4f\" %val_perf)\n",
    "        \n",
    "        # Store info about this model\n",
    "        models.append(model)\n",
    "        train_perfs.append(train_perf)\n",
    "        val_perfs.append(val_perf)\n",
    "        \n",
    "    # Model selection\n",
    "    i_best = model_selection(hparams, val_perfs, performance[\"minimize\"])\n",
    "    \n",
    "    # Instantiate a model with the selected parameters\n",
    "    best_params = hparams[i_best]\n",
    "    best_model, X_train_poly, X_val_poly, X_test_poly = polynomial_model(\n",
    "        best_params['degree'], best_params[\"regularization\"], \n",
    "        X_train, X_val, X_test\n",
    "    )\n",
    "    \n",
    "    # Evaluate the selected model\n",
    "    best_model, train_val_perf, test_perf = model_evaluation(\n",
    "        best_model, X_train_poly, Y_train, X_val_poly, Y_val, X_test_poly, Y_test, \n",
    "        performance[\"metric\"]\n",
    "    ) \n",
    "    return models, i_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: Machine learning pipeline with KFold cross-validation\n",
    "\n",
    "Here we re-use some of the building blocks defined above and complement them in order to define a simple machine learning pipeline with KFold cross-validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KFold_split(X, Y, k=5, test_ratio=0.2, seed=264):\n",
    "    \"\"\"\n",
    "    Split dataset into a test dataset and train/val kfolds\n",
    "    \"\"\"\n",
    "    # Extract test set from entire dataset\n",
    "    X_train_val, X_test, Y_train_val, Y_test = train_test_split(X, Y, test_size=test_ratio, shuffle=True, random_state=seed)\n",
    "    \n",
    "    # Create train/validation kfolds splitter\n",
    "    KFold_splitter = KFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "    X_train_folds = []\n",
    "    X_val_folds = []\n",
    "    Y_train_folds = []\n",
    "    Y_val_folds = []\n",
    "    \n",
    "    # Split train_val dataset into folds\n",
    "    for (kth_fold_train_idxs, kth_fold_val_idxs) in KFold_splitter.split(X_train_val, Y_train_val):\n",
    "        X_train_folds.append(X_train_val[kth_fold_train_idxs])\n",
    "        X_val_folds.append(X_train_val[kth_fold_val_idxs])\n",
    "        Y_train_folds.append(Y_train_val[kth_fold_train_idxs])\n",
    "        Y_val_folds.append(Y_train_val[kth_fold_val_idxs])\n",
    "        \n",
    "    print(\"Training dataset size:   \", len(X_train_folds[0]))\n",
    "    print(\"Validation dataset size: \", len(X_val_folds[0]))\n",
    "    print(\"Test dataset size:       \", len(X_test))\n",
    "    return X_train_folds, Y_train_folds, X_val_folds, Y_val_folds, X_test, Y_test\n",
    "\n",
    "def pipeline_with_KFold(\n",
    "    X, Y, hparams, performance, \n",
    "    k=5, test_ratio=0.2, seed=264\n",
    "):\n",
    "    # i. and ii.: Split dataset into a train, validation and test set\n",
    "    X_train_folds, Y_train_folds, X_val_folds, Y_val_folds, X_test, Y_test = KFold_split(\n",
    "        X, Y, k, test_ratio=test_ratio, seed=seed\n",
    "    )\n",
    "    \n",
    "    train_mean_perfs = []\n",
    "    val_mean_perfs = []\n",
    "    models = []\n",
    "    \n",
    "    # iii. Loop over sets of hyperparameters:\n",
    "    for hparam in hparams:\n",
    "        print(\"\\nUsing hyper-parameters:\", hparam)\n",
    "        \n",
    "        train_perfs = []\n",
    "        val_perfs = []\n",
    "        \n",
    "        # iv. Extra loop for the cross validation\n",
    "        for X_train, X_val, Y_train, Y_val in zip(X_train_folds, X_val_folds, Y_train_folds, Y_val_folds):\n",
    "        \n",
    "            # v. Instantiate model with current set of hyperparameter\n",
    "            model, X_train_poly, X_val_poly, X_test_poly = polynomial_model(\n",
    "                hparam['degree'], hparam[\"regularization\"], X_train, X_val, X_test\n",
    "            )\n",
    "            # Train model\n",
    "            model.fit(X_train_poly, Y_train)\n",
    "        \n",
    "            # vi. Compute train and validation loss\n",
    "            train_perf = evaluate(model, X_train_poly, Y_train, performance[\"metric\"])\n",
    "            val_perf = evaluate(model, X_val_poly, Y_val, performance[\"metric\"])\n",
    "            \n",
    "            train_perfs.append(train_perf)\n",
    "            val_perfs.append(val_perf)\n",
    "            \n",
    "        # vii. Compute mean performance for this set of hyperparameters\n",
    "        train_mean_perf = np.mean(train_perfs)\n",
    "        val_mean_perf = np.mean(val_perfs)\n",
    "        print(\"Training mean MSE:    %.4f\" %train_mean_perf)\n",
    "        print(\"Validation mean MSE: %.4f\" %val_mean_perf)\n",
    "        \n",
    "        # Store info about this set of hyperparameters\n",
    "        train_mean_perfs.append(train_mean_perf)\n",
    "        val_mean_perfs.append(val_mean_perf)\n",
    "        \n",
    "        # The model trained with the last fold will represent all other\n",
    "        # models trained with this set of hyperparameters but on other folds\n",
    "        models.append(model)\n",
    "        \n",
    "    # viii. Model selection\n",
    "    i_best = model_selection( hparams, val_mean_perfs, performance[\"minimize\"])\n",
    "    \n",
    "    # ix. Instantiate a model with the selected parameters\n",
    "    best_params = hparams[i_best]\n",
    "    best_model, X_train_poly, X_val_poly, X_test_poly = polynomial_model(\n",
    "        best_params['degree'], best_params[\"regularization\"], \n",
    "        X_train_folds[0], X_val_folds[0], X_test\n",
    "    )\n",
    "    \n",
    "    # Evaluate the selected model\n",
    "    best_model, train_val_perf, test_perf = model_evaluation(\n",
    "        best_model, \n",
    "        X_train_poly, Y_train_folds[0], \n",
    "        X_val_poly, Y_val_folds[0], \n",
    "        X_test_poly, Y_test, \n",
    "        performance[\"metric\"]\n",
    "    ) \n",
    "    return models, i_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf={\"metric\" : mean_squared_error, \"minimize\" : True}\n",
    "\n",
    "# Create the list of hyper-parameters instances:\n",
    "hyper_parameters = [\n",
    "    {\"degree\": 1, \"regularization\": 0},\n",
    "    {\"degree\": 2, \"regularization\": 0},\n",
    "    {\"degree\": 3, \"regularization\": 0},\n",
    "    {\"degree\": 4, \"regularization\": 0},\n",
    "    {\"degree\": 1, \"regularization\": 0.001},\n",
    "    {\"degree\": 2, \"regularization\": 0.001},\n",
    "    {\"degree\": 3, \"regularization\": 0.001},\n",
    "    {\"degree\": 4, \"regularization\": 0.001},\n",
    "    {\"degree\": 1, \"regularization\": 0.01},\n",
    "    {\"degree\": 2, \"regularization\": 0.01},\n",
    "    {\"degree\": 3, \"regularization\": 0.01},\n",
    "    {\"degree\": 4, \"regularization\": 0.01},\n",
    "    {\"degree\": 1, \"regularization\": 0.1},\n",
    "    {\"degree\": 2, \"regularization\": 0.1},\n",
    "    {\"degree\": 3, \"regularization\": 0.1},\n",
    "    {\"degree\": 4, \"regularization\": 0.1}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select model with regular validation:\n",
    "models, i_best = basic_pipeline(X, Y, hyper_parameters, perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select model with KFold cross-validation:\n",
    "models, i_best = pipeline_with_KFold(X, Y, hyper_parameters, perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5: Effect of regularization\n",
    "\n",
    "A singular matrix warning indicates that the matrix containing the data has columns that are approximately linearly dependant, thus the matrix becomes close to singular. \n",
    "\n",
    "Increasing the regularization hyper-parameter will eventually get rid of this warning. Indeed, recall that the Ridge model solves \n",
    "$$\\underset{w}{\\min} \\|y-Xw\\|_2^2 + \\alpha\\|w\\|_2^2$$\n",
    "Then, setting the gradient to 0 yields a unique solution: \n",
    "$$w_{sol} = (X^tX + \\alpha I)^{-1} X^ty$$\n",
    "The higher the $\\alpha$ in $X^tX + \\alpha I$, the more dominant the identity matrix term becomes compared to the potentially singular matrix term $X^tX$. When the identity matrix is dominant enough the matrix $X^tX + \\alpha I$ becomes invertible i.e. non-singular. \n",
    "\n",
    "Intuitively, the penalization term $\\alpha\\|w\\|_2^2$ forces the model to shrink the solution $w_{sol}$ toward $0$ (the bigger $\\alpha$ is, the stronger the shrinkage), which lowers the impact from irrelevant variables in the model and makes the model more resilient to columns (features) colinearity in the data matrix.\n",
    "\n",
    "In the context of this exercice, with a Ridge model of degree $3$ we may have to push the regularization hyper-parameter at a very high value ($>100$) to get rid of the singular matrix warning, and we then may get an ill-conditioned matrix warning instead. To get rid of this new warning, the regularization hyper-parameter should be set even higher.\n",
    "\n",
    "Notice that if you run the model selections several time, the cross-validation tends to be more stable than the regular validation in the sense that certain hyperparameters instances are selected quite often with cross-validation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('nglm-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 (default, Nov 24 2022, 15:19:38) \n[GCC 11.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "71c2cb666ff353b4e7b5c350d66179fa0af5c84ce239ad9fa105d94543f3ad59"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
