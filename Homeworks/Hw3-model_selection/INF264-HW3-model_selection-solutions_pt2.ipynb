{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KFold cross-validation for regression and classification\n",
    "\n",
    "## Exercise 2: Model selection for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "\n",
    "# Import custom functions for the machine learning pipeline\n",
    "from machine_learning_pipeline import split_dataset, KFold_split, evaluate, model_selection, best_model_evaluation, basic_pipeline, pipeline_with_KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Creating your own unbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_binary_dataset(ratio, n_samples=10000, seed=264):\n",
    "    \"\"\" Generate a binary dataset, \"ratio\" defining the ratio between classes\"\"\"\n",
    "    X, Y = make_classification(\n",
    "        n_samples=n_samples, \n",
    "        n_classes=2, \n",
    "        n_features=2, \n",
    "        n_redundant=0, \n",
    "        n_repeated=0, \n",
    "        weights=[ratio],\n",
    "        flip_y=0, \n",
    "        random_state=seed \n",
    "    )\n",
    "    return X, Y\n",
    "\n",
    "def plot_confusion_matrix(confusion_matrix, ax=None):\n",
    "    sns.heatmap(\n",
    "        data=confusion_matrix.round(2), annot=True, fmt='d', \n",
    "        cmap=sns.color_palette(\"RdBu_r\", 1000), ax=ax\n",
    "    )\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "def plot_scores(ratios, test_accs, test_f1_scores):\n",
    "    fig, ax = plt.subplots(tight_layout=True)\n",
    "    ax.plot(ratios, test_accs, label=\"Test accuracy\")\n",
    "    ax.plot(ratios, test_f1_scores, label=\"Test f1 scores\")\n",
    "    ax.set_xlabel(\"Ratio of 1st class instances\")\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    fig.suptitle(\"Comparison of accuracy and f1 score metrics on imbalanced datasets\")\n",
    "    fig.legend()\n",
    "    plt.show()\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratios = [0.6, 0.75, 0.9, 0.95, 0.98, 0.99]\n",
    "\n",
    "# Create the list of hyper-parameters instances:\n",
    "perf={\"metric\" : accuracy_score, \"minimize\" : False}\n",
    "hyper_parameters = [{\"n_neighbors\": i} for i in range(1,12,2)]\n",
    "n_models = len(hyper_parameters)\n",
    "model_classes = [KNeighborsClassifier]*n_models\n",
    "\n",
    "summaries = []\n",
    "test_accs = []\n",
    "test_f1_scores = []\n",
    "test_confusion_matrices = []\n",
    "for ratio in ratios:\n",
    "    print(\" ========== Current ratio: \", ratio)\n",
    "    X, Y = generate_binary_dataset(ratio)\n",
    "    \n",
    "    # Select the best knn model for this ratio using accuracy score\n",
    "    models, i_best, best_model, summary = basic_pipeline(\n",
    "        X, Y, model_classes, hyper_parameters, perf\n",
    "    )\n",
    "    \n",
    "    # Summary containing info about the entire pipeline\n",
    "    summaries.append(summary)\n",
    "    \n",
    "    # Evaluate the selected model using different metrics\n",
    "    test_accs.append(evaluate(\n",
    "        best_model, summary[\"X_test\"], summary[\"Y_test\"], accuracy_score\n",
    "    ))\n",
    "    test_f1_scores.append(evaluate(\n",
    "        best_model, summary[\"X_test\"], summary[\"Y_test\"], f1_score\n",
    "    ))\n",
    "    test_confusion_matrices.append(evaluate(\n",
    "        best_model, summary[\"X_test\"], summary[\"Y_test\"], confusion_matrix\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,3)\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    plot_confusion_matrix(test_confusion_matrices[i], ax)\n",
    "plt.show()\n",
    "    \n",
    "fig, ax = plot_scores(ratios, test_accs, test_f1_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us focus on the confusion matrices first. With a binary dataset, the confusion matrix contains the true negatives in the upper-left square, the false positives in the upper-right square, the false negatives in the bottom-left square and the true positives in the bottom-right square, where the \"positive\" class is attributed to the minority class. Equivalently, the rows of the confusion matrix represent the actual class of each sample whereas the columns represent their predicted class.\n",
    "\n",
    "Now if we look at the different confusion matrices, we notice that when data is balanced, there is a symmetry between the true positives and the true negatives, and the same can be said about the false positives with respect to the false negatives. But the more unbalanced the data, the more the symmetry collapses in the confusion matrix: true negatives (the correctly classified samples from the dominant class) converge to the total amount of samples, false positives converge to 0 and true positives are less and less prevalent until there are more false negatives than there are true positives. This shows that for the very unbalanced datasets, our $K$-NN model totally failed to capture the underlying structure of the minority class. \n",
    "\n",
    "If we consider the accuracy metric, it only captured the information that true negatives massively dominate all other categories in the presence of important data imbalance, which makes for an overall increasing accuracy as the imbalance in the data rises. In other word, the accuracy metric is not suitable to correctly assess the performance of a model when the data is unbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Using a predefined dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_custom_unbalanced_dataset(filename='custom_unbalanced_dataset.pickle'):\n",
    "    \"\"\"Load an unbalanced binary dataset\"\"\"\n",
    "    with open('custom_unbalanced_dataset.pickle', 'rb') as unbalanced_dataset:\n",
    "        X, Y = pickle.load(unbalanced_dataset)\n",
    "    return X, Y\n",
    "\n",
    "X, Y = load_custom_unbalanced_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of hyper-parameters instances:\n",
    "perf={\"metric\" : f1_score, \"minimize\" : False}\n",
    "hyper_parameters = [\n",
    "    {\"n_neighbors\": 9}, {\"n_neighbors\": 19}, \n",
    "    {}, \n",
    "    {} \n",
    "]\n",
    "\n",
    "model_classes = [\n",
    "    KNeighborsClassifier, KNeighborsClassifier,\n",
    "    DecisionTreeClassifier, \n",
    "    LogisticRegression\n",
    "]\n",
    "\n",
    "# Select the best knn model for this ratio using accuracy score\n",
    "models, i_best, best_model, summary = pipeline_with_KFold(\n",
    "    X, Y, model_classes, hyper_parameters, perf, k=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the selected model using different metrics\n",
    "test_acc = evaluate(\n",
    "    best_model, summary[\"X_test\"], summary[\"Y_test\"], accuracy_score\n",
    ")\n",
    "test_f1_score = evaluate(\n",
    "    best_model, summary[\"X_test\"], summary[\"Y_test\"], f1_score\n",
    ")\n",
    "test_confusion_matrix = evaluate(\n",
    "    best_model, summary[\"X_test\"], summary[\"Y_test\"], confusion_matrix\n",
    ")\n",
    "\n",
    "plot_confusion_matrix(test_confusion_matrix)\n",
    "print(\"Selected model test performances using different metrics:\")\n",
    "print(\"Accuracy:\", test_acc)\n",
    "print(\"f1 score:\", test_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform cross-validation, we can choose the F1-score metric, which takes into account precision and recall. Precision is the ratio of true positives among the true positives and the false positives. Recall is the ratio of true positives among the true positives and the false negatives. In the $K$-NN example above, we can see that this metric reduces significantly as the imbalance in the data increases, so it is more suitable than the accuracy metric in our case, since we are dealing once again with a very unbalanced dataset (unbalance ratio of 0.9/0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('nglm-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 (default, Nov 24 2022, 15:19:38) \n[GCC 11.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "71c2cb666ff353b4e7b5c350d66179fa0af5c84ce239ad9fa105d94543f3ad59"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
